\documentclass{article}
\usepackage[numbers]{natbib}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{./media/}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
}
\bibliographystyle{unsrtnat}
\usepackage[utf8]{inputenc}

\title{Theory Group Seminar Notes}
\author{Rishit Dagli}
\date{October 2022}

\begin{document}

\maketitle

\tableofcontents
\clearpage

\addcontentsline{toc}{section}{Introduction}
\section*{Introduction}

These are my notes for the seminars that happen in the \href{https://www.cs.toronto.edu/theory/}{Theory Group} at The University of Toronto. Many thanks to \href{http://www.cs.toronto.edu/~bor/}{Professor Allan Borodin} for allowing me to attend the Theory Group seminars and helping out.\\

A PDF of these notes is available at \url{https://rishit-dagli.github.io/cs-theory-notes/main.pdf}. An online version of these notes are available at \url{https://rishit-dagli.github.io/cs-theory-notes}.\\

The Theory Group focuses on theory of computation. The group is interested in using mathematical techniques to understand the nature of computation and to design and analyze algorithms for important and fundamental problems.\\

The members of the theory group are all interested, in one way or another, in the limitations of computation: What problems are not feasible to solve on a computer? How can the infeasibility of a problem be used to rigorously construct secure cryptographic protocols? What problems cannot be solved faster using more machines? What are the limits to how fast a particular problem can be solved or how much space is needed to solve it? How do randomness, parallelism, the operations that are allowed, and the need for fault tolerance or security affect this?

\newpage

\section{Lower Bounds for Locally Decodable Codes from Semirandom CSP Refutation}

7th October 2022\\

\noindent The related paper: Combinatorial lower bounds for 3-query LDCs by \citet{Alrabiah2022-ds}. Seminar by \href{https://www.cs.cmu.edu/~pmanohar/}{Peter Manohar}. \cite{https://doi.org/10.48550/arxiv.1911.10698} \cite{https://doi.org/10.48550/arxiv.2109.04415}

\subsection{Abstract}

A code $C$ is a q-locally decodable code (q-LDC) if one can recover any chosen bit $b_i$ of the $k$-bit message b with good confidence by randomly querying the $n$-bit encoding x on at most $q$ coordinates. Existing constructions of $2$-LDCs achieve blocklength $n = exp(O(k))$, and lower bounds show that this is in fact tight. However, when $q = 3$, far less is known: the best constructions have $n = subexp(k)$, while the best known lower bounds, that have stood for nearly two decades, only show a quadratic lower bound of $n \geq \Omega(k^2)$ on the blocklength.\\

\noindent In this talk, we will survey a new approach to prove lower bounds for LDCs using recent advances in refuting semirandom instances of constraint satisfaction problems. These new tools yield, in the $3$-query case, a near-cubic lower bound of $n \geq \tilde{\Omega}(k^3)$, improving on prior work by a polynomial factor in $k$.

\subsection{Locally Decodable Codes}

Take codes $b \in {0,1}^k \rightarrow x \in {0,1}^n$

\noindent Codes $x$ are read by the decoder, $i \in [k]$, $\hat{b_i} \in {0,1}$

\begin{definition}
$C$ is a $(q, \delta, \epsilon)$-locally decodable if for any $x$ with $\bigtriangleup(x, Enc(b)) \leq \delta n$, $Dec^x(i) = b_i$ w.p. $\geq \frac{1}{2} + \epsilon$ for any $i$.
\end{definition}

\noindent Ask the question, what is the best possible rate for a $q$-LDC given a $q$?

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 $q$ & Lower Bound & Upper Bound \\
 \hline
 2 & $2^{\Omega(k)} \leq n$ & $n \leq 2^k$ \\ 
 3 & $k^2 \leq n$ & $n \leq exp(k^{o(1)})$ \\
 $O(1)$, even & $k^{\frac{q}{q+1}} \leq n$ & $n \leq exp(k^{o(1)})$ \\ 
 $O(1)$, odd & $k^{\frac{q+1}{q-1}} \leq n$ & $n \leq exp(k^{o(1)})$ \\ 
 \hline
\end{tabular}
\end{center}

\noindent Focus on the case $q=3$, we have gotten better bounds:

\begin{equation}
k \leq n \leq 2^k
\end{equation}

\begin{equation*}
k^2 \leq n \leq exp(exp(\sqrt{\log k \log \log k}))
\end{equation*}

\noindent In \cite{Alrabiah2022-ds}, they show that a better minimum bound can be found than these existing ones for $q=3$:

\begin{equation}
k^3 \leq n
\end{equation}

\noindent The main result is that:

\begin{theorem}
Let $C$ be a $(3, \delta, \epsilon)$-locally decodable codes. Then $n \geq \tilde{\Omega}_{\delta, \epsilon}(k^3)$.
\end{theorem}

\noindent Semi-random CSP refutation comes to our aid to prove this! The intuitive way to put this theorem is that $q$-LDC lower bound is same as refuting "LDC" $q$-XOR.

\subsection{How to prove the Theorem}

The idea:

\begin{itemize}
    \item $q$-LDC lower bound is same as refuting "LDC" $q$-XOR \phantomsection\label{proofp1}
        \begin{itemize}
            \item CSP Refutation
        \end{itemize}
    \item Proof of existing $q$-LDC lower bound for $q$ even
    \item Proof sketch of $k^3$ lower bound
\end{itemize}

\subsection{Normally Decodable Codes}

We can see that the decoder we have can arbitrary but WLOG we can assume there are $q$-unif hypergraphs ${H_1, H_2, \cdots H_k}$ where every $H_i$ is such that:

\begin{equation*}
H_i \subseteq {[n] \choose q}
\end{equation*}

\noindent We can also see that:

\noindent Each $H_i$ is a matching such that $|H_i| \geq \delta n$

\noindent and, $Dec(i)$ picks $C \leftarrow H_i$ and outputs $\sum_{j \in C} x_j$\\

\noindent One such example is the Hadmard code:

\begin{equation}
b \in {0,1}^k \mapsto f = (\langle b, v \rangle)_{v \in {0,1}}^k
\end{equation}

\begin{equation*}
b_i = f(e_i) = f(v) + f(v+e_i)
\end{equation*}

\noindent Can think of this as $v$ and $v+e_i$ are connected.\\

\noindent Matching vector codes are $\approx \mathbb{Z} _{m}^{h}$

\subsection{Proof: Going from LDC to XOR}

We suppose that our code is linear and that there exists $q$-unif hypergraphs ${H_1, H_2, \cdots H_k}$.\\

\noindent We also know that:

Each $H_i$ is a matching such that $|H_i| \geq \delta n$

and, $Dec(i)$ picks $C \leftarrow H_i$ and outputs $\sum_{j \in C} x_j$\\

\noindent So, we start by considering a $q$-XOR instance $\psi_b$:

\begin{equation*}
\begin{split}
    \textit{Vars: } & \{x_j\}_{j \in [n]}\\
    \textit{Over Equations: } & \sum_{j \in C} x_j = b_i, \forall i \in [k], C \in H_i
\end{split}
\end{equation*}

\noindent We can write down the maximum fraction of satisfiable constraints: $val(\psi_b) = 1$ for any $b \in {0,1}^k$.\\

\noindent It is sufficient now if we can argue that $\psi_b$ is unsat with high probability for some random $b$ when $n \ll k^{\frac{q}{q-2}}$.\\

\noindent Now we need to refute XOR, there are many ways to argue unsatisfiability of an XOR instance. One reason why we can not use probablistic approaches here is that $\psi_b$ only has $k$ bits of randomness.\\

\noindent One way we can have some success here is to use a refutation algorithm

\begin{equation*}
\psi \rightarrow A \rightarrow algval(\psi)
\end{equation*}

\noindent With this the guarantee then would be $val(\psi) \leq algval(\psi)$ which is similar to saying that if $algval(\psi) < 1$ then $A$ refutes $\psi$. The ideal goal would be to refute random $\psi$ with $m$ constraints with high probability\\

\noindent However, we take a look at semi-random XOR. Our refutation algorithm and the guarantee will still be the same:

\begin{equation*}
\psi \rightarrow A \rightarrow algval(\psi)
\end{equation*}

\noindent with the guarantee that $val(\psi) \leq algval(\psi)$.\\

\noindent So, now we generate semi-random $\psi w/ m$ constraints:

\begin{itemize}
    \item The worst case would be random $q$-unif hypergraph
    \item Random RHS $b_c$ for each $C \in H$
\end{itemize}

\noindent The equation we have is:

\begin{equation}
    \sum_{j \in C} x_j = b_c
\end{equation}

\noindent And we also already know that 

\begin{equation*}
    \psi_b \textit{ is } \sum_{j \in C}
\end{equation*}

\noindent And, $xj = b_i, i \in [k], C \in H_i$.

\noindent $\psi_b$ is almost semi-random.\\

\noindent Thus, we have shown \ref{proofp1}{ Part 1 of Proof}.

\subsection{Proof: Existing $q$-LDC lower bound for $q$ even}

$q$-LDC XOR instance $\psi_b$ is encoded by:

\begin{itemize}
    \item $q$-uniform hypergraph matchings $\{ H_1 \cdots H_k \}$
    \item right-hand sides are random $b_i \in \{ \pm 1 \}$
    \item We have constraints $\displaystyle\prod_{j \in C} x_j=b_i$ for all $i$ and $C \in H_i$
\end{itemize}

\noindent We now have a goal to argue that $\psi_b$ unsat with high probability for random when $b$ when $n \ll k^{q/(q-2)}$\\

\noindent frac. constraints satisfied by $x \in \{ \pm 1 \}^n$ is $\frac{1}{2} + \frac{f(x)}{2}$.\\

\noindent Here $f(x)$ is:

\begin{equation}\label{eq:5}
f(x) = \frac{1}{m} \sum_{i} b_i \sum_{C \in H_i} \prod_{j \in C} x_j
\end{equation}
\begin{equation*}
m = k \cdot \delta n
\end{equation*}

\noindent This makes our goal to be to certify with high probability that:

\begin{equation}\label{eq:6}
\max_{x \in \{ \pm 1 \}^n} f(x) < 1 \textit{ when } n \ll k^{\frac{q}{q-2}}
\end{equation}

\noindent We will now try to refute $\psi_b$. With Equation \ref{eq:5} and Equation \ref{eq:6} to refute $\psi_b$ is like showing:

\begin{equation}
\textit{w.h.p. } \max_{x \in \{ \pm 1 \}^n} f(x) < 1 \textit{ where } f(x) = \frac{1}{m} \sum_{i} b_i \sum_{C \in H_i} \prod_{j \in C} x_j
\end{equation}

\noindent $\textit{ when } n \ll k^{\frac{q}{q-2}}$.\\

\noindent The idea is to design a matrix $A \in \mathbb{R}^{N \times N}$ so that:

\begin{equation*}
f(x) \leq ||A||_{\infty \to 1} = \max_{z,w \in \{ \pm 1 \}^N} z^TAw
\end{equation*}

\noindent As shown by \citet{https://doi.org/10.48550/arxiv.1904.03858} the matrix $A$ can be indexed by

\begin{equation*}
S \in {[n] \choose l}
\end{equation*}

\noindent Assign $x \mapsto y$ such that $y^TAy \propto f(x)$

\noindent and $y_s := \displaystyle\prod_{j \in S} x_j$ which is simply the tensor product.\\\

\noindent We need to now be able to answer how to set $A(S,T)$

\begin{equation}
y^TAy = \sum_{S,T} y_Sy_TA(S,T) = \sum_{S,T} A(S,T) \displaystyle\prod_{j \in S \oplus T} x_j
\end{equation}

\noindent Which shows that we are actually using symmetric difference here.\\

\noindent We say that if $S \oplus T = C \in h_i$ then $\displaystyle\prod_{j \in S \oplus T} x_j = b_i$

\noindent $\implies A(S,T) = b_i$ if $S \oplus T = C \in H_i$\\

\begin{equation}
y^TAy = \sum_{i=1}^k b_i \sum_{C \in h_i} \sum_{S \oplus t = C} \displaystyle\prod_{j \in C}x_j = Dmf(x)
\end{equation}

\noindent Here $D =$ number of $S,T$ where $S \oplus T = C$.\\

\noindent Simplifying an earlier statement we can also say from here that: $A_C(S,T) = 1$ if $S \oplus T = C$.\\

\noindent For which $A_i = \sum_{C \in h_i} A_C$ and $A = \sum_{i=1}^k b_i A_i$\\

\noindent Set $y_S := \displaystyle\prod_{j \in S} x_j$\\

\noindent $y^TAy = Dmf(x) \implies Dmf(x) \leq ||A||_{\infty \to 1}$\\

\noindent Note that the way we defined $D$ here it only depends on $|C| = q$, we can say:

\begin{equation*}
D = {q \choose \frac{q}{2}} {n-q \choose l - \frac{q}{2}}
\end{equation*}

\noindent Also we know $A_c \in \mathbb{R}^{N \times N}$ and $N = {n \choose l}$.\\

\noindent We have already proven that $||A||_{\infty \to 1} \geq Dm \max_{x}f(x) \geq Dm \geq D \delta nk$\\

\noindent It is also interesting to note that $||A||_{\infty \to 1} \leq N ||A||_2$ and we still need to be able to show that with high probability that $||A||_{\infty \to 1}$ is not too large.\\

\noindent Matrix Bernstein: with high probability over $b_i$, $||A||_2 \leq \triangle \sqrt{kl}$ where $\triangle$ is the maximum number of 1's in a row in any $A_i$.\\

\noindent Expected number of 1's per row is $\delta n \frac{D}{N} \sim n (\frac{l}{n})^{q/2}$.\\

\noindent We can optimistically suppose that $\triangle \sim n (\frac{l}{n})^{q/2}$ however this also needs $l \geq n^{1-2/q}$.\\

\noindent Then $D \cdot \delta n k \leq ||A||_{\infty \to  1} \leq N \triangle \sqrt{kl}$\\

\noindent $\implies k \leq l$ since $\triangle \sim \delta n \frac{D}{N}$\\

\noindent Now take $l=n^{1-2/q} \implies k^{q/(q-2)} \leq n$\\

\noindent So, $\triangle =\frac{2l}{q}$\\

\noindent Because $H_i$ are matchings, a random row will have only $\approx \frac{\delta n D}{N}$ 1's.\\

\noindent The idea now is to prune off all the bad rows or columns in A to get B such that:

\begin{equation*}
||A||_{\infty \to 1} \leq ||B||_{\infty \to 1} +o(N)
\end{equation*}

\noindent And, $\triangle_B \sim \delta n (\frac{l}{n})^{q/2}$\\

\noindent And now we can just use $B$ instead which will prove $q$-LDC lower bound for $q$ even.

\subsection{Proof: $k^3$ lower bound}

Recall, $q$-LDC XOR instance $\psi_b$ is encoded by:

\begin{itemize}
    \item $q$-uniform hypergraph matchings $\{ H_1 \cdots H_k \}$
    \item right-hand sides are random $b_i \in \{ \pm 1 \}$
    \item We have constraints $\displaystyle\prod_{j \in C} x_j=b_i$ for all $i$ and $C \in H_i$
\end{itemize}

\noindent The goal is argue that $\psi_b$ is unsatisfiable with high probability for random $b$. And the idea is to design a matrix $A \in \mathbb{R}^{N \times N}$ so that:

\begin{equation*}
f(x) \leq ||A||_{\infty \to 1} = \max_{z, w \in \{ \pm 1 \}^N z^TAw}
\end{equation*}

\noindent The previous approach fails because the $A$ from before requires $q$ to be even.\\

\noindent One attempt is to represent rows as $|S|=l$ and columns as $|T|=l+1$. However this will only get us to $k \leq \sqrt{n}$.\\

\noindent We need to derive more constraints, using $C_i \oplus C_j$ get us to $nk$ constraints so each $n_j$ is in $\approx k$ constraints $\implies$ new $nk^2$ constraints. \\

\noindent The matrix $A$ is indexed by $S$, $A(S,T)=b_ib_j$. The calculation is now:

\begin{equation*}
nk^2D \leq ||A||_{\infty \to 1} \leq N \triangle \sqrt{kl}
\end{equation*}

\noindent An optimist approach is $\triangle \sim Nk\frac{D}{N} = nk(\frac{l}{n})^2$\\

\noindent $\implies l \geq \sqrt{\frac{n}{k}}$\\

\noindent $\implies k \leq n \implies k^3 \leq n$\\

\noindent The row pruning tricks would still work provided that any $\{u, v\}$ is in at most $polylog(n)$ constraints.

\subsection{Conclusion}

This proof for $q=3$ is not generalizable for all odd $q$ and neither is a reduction to $2$-LDC. This is particularly true because of the row pruning step.

\newpage
\section{Algorithms for the ferromagnetic Potts model on expanders}

14th October 2022\\

\noindent The related paper: Algorithms for the ferromagnetic Potts model on expanders by \citet{https://doi.org/10.48550/arxiv.2204.01923}. Seminar by \href{https://www.adityapotukuchi.com/}{Aditya Potukuchi}.

\subsection{Abstract}

\noindent The ferromagnetic Potts model is a canonical example of a Markov random field from statistical physics that is of great probabilistic and algorithmic interest. This is a distribution over all $1$-colorings of the vertices of a graph where monochromatic edges are favored. The algorithmic problem of efficiently sampling approximately from this model is known to be \#BIS-hard, and has seen a lot of recent interest. I will outline some recently developed algorithms for approximately sampling from the ferromagnetic Potts model on d-regular weakly expanding graphs. This is achieved by a significantly sharper analysis of standard "polymer methods" using extremal graph theory and applications of Karger's algorithm to count cuts that may be of independent interest. I will give an introduction to all the topics that are relevant to the results.

\subsection{The Ferromagnetic Potts Model}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{media/sample-graph.png}
\caption{A sample graph}
\label{fig:sample-graph}
\end{figure}

\noindent We start by defining some basic notation:

\begin{itemize}
    \item $G$: finite graph on vertices $V$
    \item $q \in \mathbb{N}$, we are interested in $q$-colourings of the vertices in $G$
    \item $m(\chi)$: number of monochromatic edges induced by a colouring $\chi$
    \item Distribution on colourings given by $p(\chi) \propto exp(\beta \cdot m(\chi))$
    \item $\beta \in \mathbb{R}$: parameter, inverse temperature
\end{itemize}

\noindent Notice that for $\beta < 0$ it means that we take the antiferromagnetic case. Here we talk more about when $\beta > 0$ meaning it is ferromagnetic.\\

\noindent This could have quite some applications:

\begin{itemize}
    \item Modelling: Social networks, physics, chemistry, etc
    \item Markov Random field: Probabilistic Inference
    \item Connection to UGC \citet{coulson_et_al:LIPIcs:2020:12565}
    \item and more.
\end{itemize}

\subsection{The Problem}

\noindent we know $p(\chi) \propto exp(\beta \cdot m(\chi))$\\

\noindent Now for $\beta = 0$ it means that we are doing a uniform $q$-coloring of $V$\\

\noindent For $\beta = -\infty$ we do a uniform proper coloring of $G$\\

\noindent What we need to do is given $G$ and $\beta$, efficiently sample a coloring from this distribution.

\begin{equation}
    p(\chi) = \frac{exp(\beta m(\chi))}{\sum_{\chi}exp(\beta m(\chi))}
\end{equation}

\noindent We add the normalizing factor here:

\begin{equation*}
    \textit{Nomalizing factor } = \sum_{\chi}exp(\beta m(\chi))
\end{equation*}

\noindent Now we can also say,

\begin{equation}
    \sum_{\chi}exp(\beta m(\chi)) =: Z_G(q,\beta)
\end{equation}

\noindent A partition function of the model/distribution is very important for this POV.\\

\noindent Our problem is that given $G$ and $\beta$ we want to efficiently sample a color distribution. We give 2 facts:

\begin{enumerate}
    \item It is enough to compute $Z_G(q,\beta)$
    \item \#P-hard
\end{enumerate}

\noindent We now modify the problem as: Given $G$ and $\beta$, efficiently sample \textbf{approximately} a colouring from this distribution.\\

\noindent $\epsilon$ approximation will have us sample a law from $q$ such that $||p-q||_{TVD} \leq \epsilon$, thus

\begin{equation}
    ||p-q||_{TVD} := \frac{1}{2} \sum_{\chi} |p(\chi) - q(\chi)|
\end{equation}

\noindent We modify our original problem template to now be: Given $G$ and $\beta$, efficiently sample $\epsilon$-\textbf{approximately} a colouring from this distribution.\\

\noindent Fully Polynomial Almost Uniform Sampler can allow us to sample $\epsilon$-approximately in $poly(G,\frac{1}{\epsilon})$ time.\\

\noindent Instead Fully Polynomial Time Approximation Scheme: $1 \pm \epsilon$-factor approximation in $poly(G,\frac{1}{\epsilon})$ time.\\

\noindent We can also show for a fact that $FPTAS \iff FPAUS$.

\subsection{Antiferromagnetic Potts model}

\noindent The Antiferromagnetic Potts model:

\begin{equation}    
p(\chi) \propto exp{\beta \cdot m(\chi)}
\end{equation}

\noindent where $\beta < 0$\\

\noindent Given $G$ and $\beta < 0$, we want to be able to give an FPAUS for this distribution. It is then equivalent to instead work on the problem: given $G$ and $\beta < 0$, give an FPTAS for its partition function $Z_G(q, \beta)$.\\

\noindent From some previous work, we know that there exists a $\beta_c$ such that:

\begin{itemize}
    \item for $\beta < \beta_c$, FPTAS exists 
    \item For $\beta < \beta_c$, no FPTAS unless $NP = RP$
\end{itemize}

\noindent We can say that this is \#BIS-hard (bipartite independent sets). Thus, doing this is at least as hard as an FPTAS for the number of independent sets in bipartite graphs. If our graph has no bipartiteness then this becomes a NP-hard problem.\\

\noindent For now, let's consider the problem given a bipartite graph $G$, design an FPTAS for the number of individual sets in $G$. This accurately captures the difficulty of: the number of proper $q$-colorings of a bipartite graph for $q \geq 3$, the number of stable matchings, the number of antichains in posets.

\subsection{Main Results}

\noindent For our purposes we assume that $G$ is always a $d$-regular graph on $n$ vertices. Now for a set $S \subset V$, we define it's edge boundary as:

\begin{equation*}
    \triangledown(S) := \# (uv \in G | u \in S, v \notin S)
\end{equation*}

\noindent Now, $G$ is an $\eta$ expander if for every $S \subset V$ of size at most $n/2$, we have $|\triangledown(S)| \geq \eta|S|$. For example we can take a discrete cube $Q_d$ with vertices $\{0,1\}^d$, $uv$ is an edge if $u$ and $v$ differ in exactly 1 coordinate.

\noindent Using a simplification of the Harper's Theorem we can say that $Q_d$ is a $1$-expander \cite{frankl1981short}.\\

\begin{theorem}
For each $\epsilon > 0$ and there is a $d=d(\epsilon)$ and $q = q(\epsilon)$ such that there is an FPTAS for $Z_G(q, \beta)$ where $G$ is a $d$-regular $2$-expander providing the following conditions hold:

\begin{itemize}
    \item $q=poly(d)$
    \item $\beta \notin (2 \pm \epsilon)\frac{ln(q)}{d}$
\end{itemize}
\end{theorem}

\noindent The main result shown was that 

\begin{theorem}
For each $\epsilon > 0$, and $d$ large enough, there is an FPTAS for $Z_G(q, \beta)$ where $G$ for the class of $d$-regular triangle-free $1$-expander grpahs providing the following conditions hold:

\begin{itemize}
    \item $q \geq poly(d)$
    \item $\beta \notin (2 \pm \epsilon)\frac{ln(q)}{d}$
\end{itemize}
\end{theorem}

\noindent This was previously known for:

\begin{itemize}
    \item Stronger expansion and $d = q^{\Omega(d)}$
    \item Higher temperature and $q = d^{\Omega(d)}$
\end{itemize}

\noindent Something to note here is that $q \geq poly(d)$ should not be a necessary condition.\\

\noindent As well as as in the case $\beta \leq (1-\epsilon) \beta_0$ does not require expansion or even that $q \geq poly(d)$.

\subsection{Potts Distribution}

\noindent We first write the order-disorder threshold of the ferromagnetic Potts model

\begin{equation}
    \begin{split}
    \beta_0 :=& ln(\frac{q-2}{(q-1)^{1-2/d} - 1})\\
    \beta_0 =& 2 \frac{ln q}{d} (1+O(\frac{1}{q}))
    \end{split}
\end{equation}

\noindent We want to be able to know more about how the Potts distribution looks for $\beta < (1-\epsilon)\beta_0$ and for $\beta > (1+\epsilon)\beta_0$

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{media/potts-model.jpg}
\caption{Rough picture of the Potts model}
\label{fig:potts-model}
\end{figure}

\subsection{Results}

Another result we have is:

\begin{theorem}
    For each $\epsilon>0$, let $d$ be large enough $q \geq poly(d)$, and $G$ be a $d$-regular $2$-expander graph on $n$ vertices then,

    \begin{itemize}
        \item For $\beta < (1-\epsilon) \beta_0$, every colour class has size $n/q (1 \pm o(1))$ with high probability
        \item For $\beta > (1+\epsilon) \beta_0$, every colour class has size $n-o(n)$ with high probability
    \end{itemize}
\end{theorem}

\noindent The strategy we have, to prove the theorem for $\beta < (1-\epsilon) \beta_0$:

\begin{itemize}
    \item Pass to the Random Cluster Model
    \item Distribution on subsets of edges: $p(A) \propto q^{k(A)} (e^{\beta}-1)^{|A|}$
    \item $Z_G^{RC}(q, \beta) = Z_G^{Potts}(q, \beta)$
    \item Sampling algorithm: Sample from random cluster model, give each connected component a uniform color
    \item Standard polymer methods + careful enumeration
\end{itemize}

\subsection{Polymer Methods}

\noindent The motivating idea is to visualize the state for $\beta$ large at low temperature as ground state + defects.\\

\noindent Typical Colouring = Ground State + Defects\\

\noindent Polymer methods are pretty useful in such cases. These were first proposed in \cite{10.1145/3313276.3316305} and originated in statistical physics. We take $G$ to be our defect graph and each node in this represents a defect.\\

\noindent Now using Polymer methods $X \sim _GY$

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{media/proof-schematic.jpg}
\caption{Proof Schematic}
\label{fig:proof-schematic}
\end{figure}

\noindent Ideas is to $Z_G(q,\beta) \sim Z_{red} + Z_{blue}+\dots$ where $Z_{red} \approx e^{\beta nd/2}$\\

\noindent $Z_{red} e^{-\beta nd/2} = \sum_{I \subset V(G)} \prod_{\gamma \in I}w_{\gamma}$ where $w_{\gamma}$ is the weight of polymer $\gamma$.\\

We now move towards cluster expansion: multivariate in the $w_{\gamma}$ Taylor expansion of:

\begin{equation*}
    ln(\sum_{I \subset V(G)} \prod_{\gamma \in I}w_{\gamma})
\end{equation*}

\noindent This is an infinite sum, so convergence is not guaranteed however convergence can be established by verifying the Kotecký-Preiss criterion.\\

\noindent We also want to answer how many connected subsets are there of a given edge boundary in an $\eta$-expander?

\noindent A heuristic we have is to count the number of such subsets that contain a given vertex $u$: a typical connected subgraph of size $a$ is tree-like, i.e., has edge boundary $a \cdot d$.\\

\noindent Working backward, a typically connected subgraph with edge boundary size $b$ has $O(b/d)$ vertices. The number of such subgraphs $\leq$ number of connected subgraphs of size $O(b/d)$ containing $u$. The original number of subsets is also $\leq$ Number of rooted (at $u$) trees with $O(b/d)$ vertices and maximum degree at most $d = d^{O(b/d)}$. Thus,

\begin{theorem}
    At most $d^{O(1+1/\eta)b/d}$ connected subsets in an $\eta$ expander that contains $u$ have edge boundary of size at most $b$.
\end{theorem}

\noindent Another question to ask is how many $q$-colorings of an $\eta$-expander induce at most $k$ non-monochromatic edges?\\

\noindent  Easiest way is to make $k$ non-monochrimatic edges is to color all but $k/d$ randomly chosen vertices with the same color. Now, $k$ small $\implies$ these vertices likely form an independent set. we now color these $k/d$ vertices arbitrarily. There are:

\begin{equation*}
    {n \choose k/d} q^{k/d+1}
\end{equation*}

\noindent ways.

\begin{theorem}
    For $\eta$-expanders and $q \geq poly(d)$ there are at most $n^4 q^{O(k/d)}$ possible colourings.
\end{theorem}

\noindent Now we also know the maximum value of $Z_G(q,\beta)$ over all graphs $G$ with $n$ vertices, $m$ edges, and max degree $d$. This will always be attained when $G$ is a disjoint union of $K_{d+1}$ and $K_1$

\newpage
\section{Statistical Learning using Compression}

18th October 2022\\

\noindent The related paper: Adversarially Robust Learning with Tolerance by \citet{https://doi.org/10.48550/arxiv.2203.00849}. Seminar by \href{https://www.cas.mcmaster.ca/ashtiani/}{Hassan Ashtiani}.

\subsection{Abstract}

\noindent Characterizing the sample complexity of different machine learning tasks is one of the central questions in statistical learning theory. For example, the classic Vapnik-Chervonenkis theory characterizes the sample complexity of binary classification. Despite this early progress, the sample complexity of many important learning tasks — including density estimation and learning under adversarial perturbations — are not yet resolved. In this talk, we review the less conventional approach of using compression schemes for proving sample complexity upper bounds, with specific applications in learning under adversarial perturbations and learning Gaussian mixture models.

\subsection{Some background}

\noindent We start by defining some notation:

\begin{itemize}
    \item $Z$: domain set
    \item $D_Z$: distribution over $Z$
    \item $S$: i.i.d sample from $D_Z$
    \item $H$: class of models/hypotheses
    \item $L(D_Z,H) \to \mathbb{R}$: loss/ error function
    \item $OPT=inf_{h \in H} L(D_z,h)$: best achievable
    \item $A_{Z,H}: Z^* \to H$: learner
\end{itemize}

\subsection{Density Estimation}

\noindent Our goal is that for every $D_Z$, $A_{Z,H}(S)$ we want it to be comparable to $OPT$ with high probability.\\

\noindent We take the example of density estimation in this case $L(D_Z, h) = d_{TV} (D_z, h)$. Now, $A_{Z,H}$ probably approximately correct learns $H$ with $m(\epsilon, \delta)$ samples if for all $D_Z$ and for all $\epsilon$ with a $\delta \in ( 0,1 )$. Now if $S \sim D_Z^{m(\epsilon, \delta)}$ then:

\begin{equation}
    \underset{S}{\mathrm{Pr}} [L(D_Z, A_{Z,H}(S)) > \epsilon + C \cdot OPT] < \delta
\end{equation}

\noindent Now if we take the example of $C=2$, let $H$ be the set of all Gaussians in $\mathbb{R}^d$ then:

\begin{equation*}
    m(\epsilon, \delta) = O \left( \frac{d^2 + \log 1/\delta}{\epsilon^2} \right)
\end{equation*}

\noindent We will now modify the above equation. Now, $A_{Z,H}$ probably approximately correct learns $H$ with $m(\epsilon)$ samples if for all $D_Z$ and for all $\epsilon \in ( 0,1 )$. Now if $S \sim D_Z^{m(\epsilon)}$ then:

\begin{equation}
    \underset{S}{\mathrm{Pr}} [L(D_Z, A_{Z,H}(S)) > \epsilon + C \cdot OPT] < 0.01
\end{equation}

\noindent For the example of $C=2$, let $H$ be the set of all Gaussians in $\mathbb{R}^d$ then:

\begin{equation*}
    m(\epsilon, \delta) = O \left( \frac{d^2}{\epsilon^2} \right)
\end{equation*}

\subsection{Binary Classification (with adv. pertubations)}

\noindent For the example of binary classification, we have $Z = X \times \{ 0, 1 \}$ and $h$ is some model which maps from $h: X \to \{ 0, 1 \}$.\\

\noindent We also have $l(h,x,y) = 1 {h(x) \neq y}$ and then we will have the $L$ be $L(D_Z,h) = E_{(x,y) \sim D_Z} l(h,x,y)$.\\

\noindent Now, $A_{Z,H}$ probably approximately correct learns $H$ with $m(\epsilon)$ samples if for all $D_Z$ and for all $\epsilon \in ( 0,1 )$. Now if $S \sim D_Z^{m(\epsilon)}$ then:

\begin{equation}
    \underset{S}{\mathrm{Pr}} [L(D_Z, A_{Z,H}(S)) > \epsilon + C \cdot OPT] < 0.01
\end{equation}

\noindent Now $H$ is the set of all half spaces in $\mathbb{R}^d$ then:

\begin{equation*}
    m(\epsilon) = O \left( \frac{d}{\epsilon^2} \right)
\end{equation*}

\noindent For the example of binary classification, we have $Z = X \times \{ 0, 1 \}$ and $h$ is some model which maps from $h: X \to \{ 0, 1 \}$.\\

\noindent We also have $l^U(h,x,y) =$ adversarial pertubations and then we will have the $L^U$ be $L^U(D_Z,h) = E_{(x,y) \sim D_Z} l^U(h,x,y)$.\\

\noindent Now, $A_{Z,H}$ probably approximately correct learns $H$ with $m(\epsilon)$ samples if for all $D_Z$ and for all $\epsilon \in ( 0,1 )$. Now if $S \sim D_Z^{m(\epsilon)}$ then:

\begin{equation}
    \underset{S}{\mathrm{Pr}} [L^U(D_Z, A_{Z,H}(S)) \epsilon + OPT] < 0.01
\end{equation}

\noindent Now, consider the following $H_1$ and $H_2$:

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{media/trade-off.png}
\caption{Trade Offs}
\label{fig:trade-off}
\end{figure}

\noindent Here $H_2$ is richer which can make it contain better models as well as harder to learn. We can characterize the sample complexity of learning $H$ using Binary classification, with Binary classification with
adversarial perturbations or with Density estimation.\\

\noindent In the case of binary classification the VC-dimension quantifies complexity:

\begin{equation*}
    m ( \epsilon ) = \Theta \left( \frac{VC(H)}{\epsilon^2} \right)
\end{equation*}

\noindent The upper bound here is achieved using simple ERM

\begin{equation*}
    L(S,h) = \frac{1}{|S|} \sum_{(x,y)\in S} l(h,x,y)
\end{equation*}

\begin{equation*}
    h = argmin_{h \in H} L(S,h)
\end{equation*}

\noindent And then for uniform convergence:

\begin{equation}
    sup_{h \in H} |L(S,h) - L(D_z,h)| = O \left( \sqrt{\frac{VC(H)}{|S|}} \right) w.p. > 0.99
\end{equation}

We now introduce sample compression as an alternative.

\subsection{Sample Compression}

\noindent The idea is to try and answer how should we go about compressing a given training set? In classic information theory, we would compress it into a few bits. In the case of sample compression, we want to try to compress it into a few samples.\\

\noindent If we just take the simple example of linear classification Number of required bits is unbounded (depends on the sample).\\

\noindent It has already been shown by \citet{littlestone1986relating} that Compressibility $\implies$ Learnability

\begin{equation*}
    m(\epsilon)=\Tilde{O} \left( \frac{k}{\epsilon^2} \right)
\end{equation*}

\noindent It has also been shown by \citet{moran2016sample} Compressibility $\impliedby$ Learnability

\begin{equation*}
    k = 2^{O(VC)}
\end{equation*}

\begin{conjecture}[Compression Conjecture]
    $k = \Theta(VC)$
\end{conjecture}

\noindent Sample Compression can be very helpful by:

\begin{itemize}
    \item being simpler and more intuitive
    \item being more generic. It can work even if uniform convergence fails! Can show optimal SVM bound and we can also perform compression for learning under Adversarial Perturbations.
\end{itemize}

\noindent Typical classifiers are often:

\begin{itemize}
    \item Sensitive to “adversarial” perturbations, even when the noise is “imperceptible”
    \item Vulnerable to malicious attacks
    \item Ignore the “invariance” or domain-knowledge
\end{itemize}

\noindent In the case of classification with adversarial perturbations we had $l^{0/1}(h,x,y) = 1 \{h(x) \neq y\}$ and $l^U(h,x,y) = sup_{\Bar{x} \in U(x)} l^{0/1}(h,\Bar{x},y)$\\

\noindent and then we will have the $L^U$ be $L^U(D_Z,h) = E_{(x,y) \sim D_Z} l^U(h,x,y)$.\\

\noindent Now, $A_{Z,H}$ probably approximately correct learns $H$ with $m(\epsilon)$ samples if for all $D_Z$ and for all $\epsilon \in ( 0,1 )$. Now if $S \sim D_Z^{m(\epsilon)}$ then:

\begin{equation}
    \underset{S}{\mathrm{Pr}} [L^U(D_Z, A_{Z,H}(S)) \epsilon + OPT] < 0.01
\end{equation}

\noindent However one of the problems with this is if the robust ERM works for all $H$

\begin{equation*}
    L(S,h) = \frac{1}{|S|} \sum_{(x,y)\in S} l(h,x,y)
\end{equation*}

\begin{equation*}
    h = argmin_{h \in H} L(S,h)
\end{equation*}

\noindent The robust ERM would not work for all $H$, uniform convergence can fail,

\begin{equation*}
    sup_{h \in H} |L^U(S,h) - L^U(D_z, h)|
\end{equation*}

\noindent can be unbounded.\\

\noindent We can say that any “proper learner” (outputs from $H$) can fail.\\

\noindent In a compression-based method the decoder should recover the labels
of the training set and their neighbors and then compress the inflates set:

\begin{equation*}
    k = 2^{O(VC)}
\end{equation*}

\noindent So,

\begin{equation}
    m^U(\epsilon) = O \left( \frac{2^{VC(H)}}{\epsilon^2} \right)
\end{equation}

\noindent There is an exponential dependence on $VC(H)$.\\

\noindent \citet{https://doi.org/10.48550/arxiv.2203.00849} introduced tolerant adversarial learning $A_{Z,H}$ PAC learns $H$ with $m(\epsilon)$ samples\\

\noindent if $\forall D_Z$, $\forall \epsilon \in (0,1)$, if $S \simeq D_Z^{m(\epsilon)}$ then

\begin{equation}
    Pr_S[L^U(D_Z,A_{Z,H}(S)) > \epsilon + inf_{h \in H} L^V(D_Z,A_{Z,H}(S))] < 0.01
\end{equation}

\noindent And,

\begin{equation}
    m^{U,V}(\epsilon) = \Tilde{O} \left( \frac{VC(H)d\log(1+\frac{1}{\gamma})}{\epsilon ^2} \right)
\end{equation}

\noindent The trick is to avoid compressing an infinite set and now our new goal is that the decoder should only recover labels of things in $U(x)$.\\

\noindent To do so we can define a noisy empirical distribution (using $V(x)$) and then use boosting to achieve a super small error with respect to this distribution. And then, we encode the classifier using the samples used to train weak learners and the decoder smooths out the hypotheses.\\

\noindent It is interesting to think of Why do we need tolerance? There do exist some other ways to relax the problem and avoid $2^{O(VC)}$

\begin{itemize}
    \item bounded adversary
    \item Limited black-box query access to the hypothesis
    \item Related to the certification problem
\end{itemize}

\noindent This is also observable in the density estimation example.

\subsection{Gaussian Mixture Models}

Gaussian mixture Models are very popular in practice and are one of the most basic universal density approximators. These are also the building blocks for more sophisticated density classes and can think of them as multi-modal versions of Gaussians.

\begin{equation*}
    f(x) = w_1N(x|\mu_1,\sum 1)+w_2N(x|\mu_2,\sum 2)+w_3N(x|\mu_3,\sum 3)
\end{equation*}

\noindent We say $F$ is Gaussian Mixture Model with $k$ components in $\mathbb{R}^d$. And we want to ask how many samples is needed to recover $f \in F$ within $L_1$ error $\epsilon$.\\

\noindent The number of samples $\simeq m(d,k,\epsilon)$.\\

\noindent To learn single Gaussian in $\mathbb{R}^d$ then

\begin{equation*}
    O \left( \frac{d^2}{\epsilon^2} \right) = O \left( \frac{\# params}{\epsilon^2} \right)
\end{equation*}

\noindent samples are sufficient (and necessary).\\

\noindent Now if we have $k$ Gaussian in $\mathbb{R}^d$ then we want to know if 

\begin{equation*}
    O \left( \frac{kd^2}{\epsilon^2} \right) = O \left( \frac{\# params}{\epsilon^2} \right)
\end{equation*}

\noindent samples are sufficient?\\

\noindent There have been some results on learning Gaussian Mixture Models.\\

\noindent Let us take the example of this graph. For a moment look at this as a binary classification problem. The decision boundary has a simple quadratic form!

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{media/example-problem.png}
\caption{A sample problem}
\label{fig:ex-problem}
\end{figure}

\begin{equation*}
    VC-dim=O(D^2)
\end{equation*}

\noindent Here “Sample compression" does npt make sense as there are no “labels”.

\subsection{Compression Framework}

\noindent We have $F$ which is a class of distributions (e.g. Gaussians) and we have. If A sends $t$ points from $m$ points and B approximates $D$
then we say $F$ admits $(t,m)$-compression.

\begin{theorem}
    If $F$ has a compression scheme of size $(t,m)$ then sample complexity of learning $F$ is
    \begin{equation*}
        \Tilde{O} \left( \frac{t}{\epsilon^2} + m \right)
    \end{equation*}
    $\Tilde{O}(\cdot)$ hides polylog factors.
\end{theorem}

\noindent Small compression schemes imply sample-efficient algorithms.

\begin{theorem}
    If $F$ has a compression scheme of size $(t, m)$ then $k$ mixtures of $F$ admits $(kt,km)$ compression.
\end{theorem}

\noindent Distribution compression schemes extend to mixture classes automatically! So for the case of GMMs in $\mathbb{R}^d$ it is enough to come up with a good compression scheme for a single Gaussian!\\

\noindent For learning mixtures of Gaussians, the encoding center and axes of ellipsoid is sufficient to recover $N(\mu,\Sigma)$. This admits $\Tilde{O}(d^2,\frac{1}{\epsilon})$ compression! The technical challenge is encoding the $d$ eigenvectors “accurately” using only $d^2$ points.\\

\noindent $\frac{\sigma_{max}}{\sigma_{min}}$ can be large which is a technical challenge.

\subsection{Conclusion}

\begin{itemize}
    \item Compression is simple, intuitive, generic
    \item Compression relies heavily on a few points
    \begin{itemize}
        \item But still can give “robust” methods
        \item Agnostic sample compression
        \item Robust target compression
    \end{itemize}
    \item Target compression is quite general
    \begin{itemize}
        \item Reduces the problem to learning from finite classes
        \item Does it characterize learning?
    \end{itemize}
\end{itemize}

\newpage
\bibliography{references}

\end{document}
